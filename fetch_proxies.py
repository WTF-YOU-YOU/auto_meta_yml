"""
fetch_proxies.py
================
ä» addr.txt ä¸­è¯»å–è®¢é˜…é“¾æ¥ï¼ŒæŠ“å–å¹¶æå–ä»£ç†èŠ‚ç‚¹ï¼Œ
æµ‹è¯•æ¯ä¸ªèŠ‚ç‚¹åˆ° www.google.com çš„ TCP è¿é€šæ€§å»¶è¿Ÿï¼Œ
åˆ é™¤è¶…æ—¶ä¸å¯è¾¾èŠ‚ç‚¹ï¼Œæœ€ç»ˆç”Ÿæˆå¯åœ¨ Clash Verge (Meta å†…æ ¸) ä¸­ä½¿ç”¨çš„ outcome.meta.yml é…ç½®æ–‡ä»¶ã€‚

ç”¨æ³•ï¼š
    python fetch_proxies.py

ä¾èµ–ï¼š
    pip install requests pyyaml
"""

import re
import sys
import time
import socket
import logging
from pathlib import Path
from datetime import datetime
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed

import yaml
import requests

# ======================== æ—¥å¿—é…ç½® ========================
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger(__name__)

# ======================== è·¯å¾„å®šä¹‰ ========================
BASE_DIR = Path(__file__).resolve().parent
ADDR_FILE = BASE_DIR / "addr.yaml"          # è®¢é˜…é“¾æ¥æ–‡ä»¶ (YAML æ ¼å¼)
OUTPUT_FILE = BASE_DIR / "outcome.meta.yml"  # è¾“å‡ºæ–‡ä»¶

# ======================== ç½‘ç»œè¯·æ±‚é…ç½® ========================
REQUEST_TIMEOUT = 30   # å•ä¸ªè®¢é˜…è¯·æ±‚è¶…æ—¶ç§’æ•°
MAX_RETRIES = 2        # å¤±è´¥é‡è¯•æ¬¡æ•°
HEADERS = {
    "User-Agent": "ClashMetaForAndroid/2.10.1.Meta Mihomo/1.18"
}

# ======================== å»¶è¿Ÿæµ‹è¯•é…ç½® ========================
LATENCY_TEST_HOST = "www.google.com"  # å»¶è¿Ÿæµ‹è¯•ç›®æ ‡ä¸»æœº
LATENCY_TEST_PORT = 443               # å»¶è¿Ÿæµ‹è¯•ç›®æ ‡ç«¯å£ (HTTPS)
LATENCY_TIMEOUT = 3                   # å•èŠ‚ç‚¹å»¶è¿Ÿæµ‹è¯•è¶…æ—¶ç§’æ•°ï¼ˆè¶…è¿‡3ç§’ç›´æ¥åˆ é™¤ï¼‰
LATENCY_MAX_WORKERS = 50              # å¹¶å‘æµ‹è¯•çº¿ç¨‹æ•°

# ======================== èŠ‚ç‚¹è¿‡æ»¤é…ç½® ========================
# éœ€è¦æ’é™¤çš„åœ°åŒºå…³é”®è¯ï¼ˆä¸­å›½å¤§é™†ã€é¦™æ¸¯ã€å°æ¹¾ï¼‰
BLOCKED_REGION_PATTERN = re.compile(
    r"(?:ğŸ‡¨ğŸ‡³|ğŸ‡­ğŸ‡°|ğŸ‡¹ğŸ‡¼|ğŸ‡»ğŸ‡³|CN|HK|TW|VN|ä¸­å›½|é¦™æ¸¯|å°æ¹¾|è¶Šå—|China|Hong.?Kong|Taiwan|Vietnam|å›å›½)",
    re.I,
)

# ======================== å›½å®¶/åœ°åŒºåˆ†ç±»æ˜ å°„ ========================
# å…³é”®è¯ â†’ (ç»„å, æ’åºæƒé‡)  æƒé‡è¶Šå°è¶Šé å‰
REGION_MAP: list[tuple[re.Pattern, str]] = [
    (re.compile(r"(?:ğŸ‡¯ğŸ‡µ|JP|æ—¥æœ¬|Japan|ä¸œäº¬|å¤§é˜ª)", re.I), "ğŸ‡¯ğŸ‡µ æ—¥æœ¬"),
    (re.compile(r"(?:ğŸ‡ºğŸ‡¸|US|ç¾å›½|United.?States|æ´›æ‰çŸ¶|ç¡…è°·|çº½çº¦|è¾¾æ‹‰æ–¯|å‡¤å‡°åŸ|è¥¿é›…å›¾)", re.I), "ğŸ‡ºğŸ‡¸ ç¾å›½"),
    (re.compile(r"(?:ğŸ‡¸ğŸ‡¬|SG|æ–°åŠ å¡|Singapore|ç‹®åŸ)", re.I), "ğŸ‡¸ğŸ‡¬ æ–°åŠ å¡"),
    (re.compile(r"(?:ğŸ‡°ğŸ‡·|KR|éŸ©å›½|Korea|é¦–å°”)", re.I), "ğŸ‡°ğŸ‡· éŸ©å›½"),
    (re.compile(r"(?:ğŸ‡¬ğŸ‡§|UK|GB|è‹±å›½|United.?Kingdom|ä¼¦æ•¦)", re.I), "ğŸ‡¬ğŸ‡§ è‹±å›½"),
    (re.compile(r"(?:ğŸ‡©ğŸ‡ª|DE|å¾·å›½|Germany|æ³•å…°å…‹ç¦)", re.I), "ğŸ‡©ğŸ‡ª å¾·å›½"),
    (re.compile(r"(?:ğŸ‡«ğŸ‡·|FR|æ³•å›½|France|å·´é»)", re.I), "ğŸ‡«ğŸ‡· æ³•å›½"),
    (re.compile(r"(?:ğŸ‡·ğŸ‡º|RU|ä¿„ç½—æ–¯|Russia|è«æ–¯ç§‘)", re.I), "ğŸ‡·ğŸ‡º ä¿„ç½—æ–¯"),
    (re.compile(r"(?:ğŸ‡¨ğŸ‡¦|CA|åŠ æ‹¿å¤§|Canada)", re.I), "ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§"),
    (re.compile(r"(?:ğŸ‡¦ğŸ‡º|AU|æ¾³å¤§åˆ©äºš|Australia|æ‚‰å°¼)", re.I), "ğŸ‡¦ğŸ‡º æ¾³å¤§åˆ©äºš"),
    (re.compile(r"(?:ğŸ‡®ğŸ‡³|IN|å°åº¦|India|å­Ÿä¹°)", re.I), "ğŸ‡®ğŸ‡³ å°åº¦"),
    (re.compile(r"(?:ğŸ‡§ğŸ‡·|BR|å·´è¥¿|Brazil)", re.I), "ğŸ‡§ğŸ‡· å·´è¥¿"),
]

# æ‰€æœ‰å¯èƒ½å‡ºç°çš„åœ°åŒºç»„åï¼ˆå›ºå®šé¡ºåºï¼‰
ALL_REGION_NAMES = [item[1] for item in REGION_MAP]


# ======================== å·¥å…·å‡½æ•° ========================

def read_urls(filepath: Path) -> list[str]:
    """
    ä» YAML é…ç½®æ–‡ä»¶ä¸­è¯»å–è®¢é˜… URL åˆ—è¡¨ã€‚
    æœŸæœ›æ ¼å¼:
        urls:
          - https://example.com/sub1.yaml
          - https://example.com/sub2.yaml
    """
    if not filepath.exists():
        log.error(f"è®¢é˜…é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        sys.exit(1)

    try:
        data = yaml.safe_load(filepath.read_text(encoding="utf-8"))
    except yaml.YAMLError as e:
        log.error(f"è®¢é˜…é“¾æ¥æ–‡ä»¶ YAML è§£æå¤±è´¥: {e}")
        sys.exit(1)

    if not isinstance(data, dict) or "urls" not in data:
        log.error("è®¢é˜…é“¾æ¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ 'urls' å­—æ®µ")
        sys.exit(1)

    raw_urls = data["urls"]
    if not isinstance(raw_urls, list):
        log.error("'urls' å­—æ®µå¿…é¡»ä¸ºåˆ—è¡¨")
        sys.exit(1)

    # è¿‡æ»¤æœ‰æ•ˆçš„ URL
    urls = [str(u).strip() for u in raw_urls
            if isinstance(u, str) and str(u).strip().startswith("http")]

    log.info(f"ä» {filepath.name} ä¸­è¯»å–åˆ° {len(urls)} æ¡è®¢é˜…é“¾æ¥")
    return urls


def fetch_content(url: str) -> str | None:
    """
    è¯·æ±‚è®¢é˜…é“¾æ¥å¹¶è¿”å›å“åº”æ–‡æœ¬ã€‚
    æ”¯æŒé‡è¯•æœºåˆ¶ï¼Œè¿”å› None è¡¨ç¤ºè·å–å¤±è´¥ã€‚
    """
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            log.info(f"  æ­£åœ¨æŠ“å– ({attempt}/{MAX_RETRIES}): {url[:80]}...")
            resp = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            return resp.text
        except requests.RequestException as e:
            log.warning(f"  è¯·æ±‚å¤±è´¥: {e}")
            if attempt < MAX_RETRIES:
                time.sleep(2)
    return None


def extract_proxies(raw_text: str) -> list[dict]:
    """
    ä» YAML æ–‡æœ¬ä¸­æå– proxies åˆ—è¡¨ã€‚
    å…¼å®¹ä¸¤ç§æ ¼å¼ï¼š
      1. å®Œæ•´ Clash é…ç½®ï¼ˆå« proxies å­—æ®µï¼‰
      2. proxy-provider æ ¼å¼ï¼ˆé¡¶å±‚å°±æ˜¯ proxies åˆ—è¡¨ï¼‰
    """
    try:
        data = yaml.safe_load(raw_text)
    except yaml.YAMLError as e:
        log.warning(f"  YAML è§£æå¤±è´¥: {e}")
        return []

    if not isinstance(data, dict):
        return []

    # æƒ…å†µ1: æ ‡å‡† Clash é…ç½®
    if "proxies" in data and isinstance(data["proxies"], list):
        return data["proxies"]

    # æƒ…å†µ2: proxy-provider æ ¼å¼ (payload å­—æ®µ)
    if "payload" in data and isinstance(data["payload"], list):
        return data["payload"]

    return []


def deduplicate_proxies(all_proxies: list[dict]) -> list[dict]:
    """
    å»é‡ä»£ç†èŠ‚ç‚¹ï¼ˆä»¥ name ä¸ºä¸»é”®ï¼‰ã€‚
    å½“å‡ºç°é‡åèŠ‚ç‚¹æ—¶ï¼Œåœ¨åç§°åè¿½åŠ åºå·ã€‚
    åŒæ—¶è¿‡æ»¤æ‰ç¼ºå°‘å¿…è¦å­—æ®µçš„æ— æ•ˆèŠ‚ç‚¹ã€‚
    """
    seen_names: dict[str, int] = {}  # name -> å‡ºç°æ¬¡æ•°
    result: list[dict] = []

    for proxy in all_proxies:
        # åŸºæœ¬æœ‰æ•ˆæ€§æ ¡éªŒ
        if not isinstance(proxy, dict):
            continue
        if "name" not in proxy or "type" not in proxy or "server" not in proxy:
            continue

        name = str(proxy["name"]).strip()
        if not name:
            continue

        # å¤„ç†é‡å
        if name in seen_names:
            seen_names[name] += 1
            name = f"{name}_{seen_names[name]}"
        else:
            seen_names[name] = 0

        proxy["name"] = name
        result.append(proxy)

    return result


def filter_blocked_regions(proxies: list[dict]) -> list[dict]:
    """
    è¿‡æ»¤æ‰å±äºè¢«å±è”½åœ°åŒºï¼ˆä¸­å›½å¤§é™†ã€é¦™æ¸¯ã€å°æ¹¾ï¼‰çš„èŠ‚ç‚¹ã€‚
    é€šè¿‡èŠ‚ç‚¹åç§°ä¸­çš„å…³é”®è¯è¿›è¡ŒåŒ¹é…ã€‚
    """
    result: list[dict] = []
    blocked_count = 0

    for proxy in proxies:
        name = proxy.get("name", "")
        if BLOCKED_REGION_PATTERN.search(name):
            blocked_count += 1
        else:
            result.append(proxy)

    if blocked_count > 0:
        log.info(f"å·²è¿‡æ»¤ {blocked_count} ä¸ªä¸­å›½å¤§é™†/é¦™æ¸¯/å°æ¹¾/è¶Šå—èŠ‚ç‚¹ï¼Œå‰©ä½™ {len(result)} ä¸ª")

    return result


def test_single_proxy(proxy: dict) -> tuple[dict, float | None]:
    """
    æµ‹è¯•å•ä¸ªä»£ç†èŠ‚ç‚¹çš„è¿é€šæ€§å»¶è¿Ÿã€‚
    é€šè¿‡ TCP è¿æ¥åˆ°èŠ‚ç‚¹çš„ server:portï¼Œå†ç»ç”±è¯¥è¿æ¥å°è¯•åˆ°è¾¾ www.google.com:443ã€‚
    ç”±äºæ— æ³•åœ¨è„šæœ¬å±‚é¢å®ç°å„ç§ä»£ç†åè®®çš„æ¡æ‰‹ï¼Œè¿™é‡Œé‡‡ç”¨ä¸¤æ­¥ç­–ç•¥ï¼š
      1. TCP è¿æ¥åˆ°ä»£ç†æœåŠ¡å™¨çš„ server:portï¼ˆéªŒè¯æœåŠ¡å™¨æ˜¯å¦å¯è¾¾ï¼‰
      2. è¿”å› TCP è¿æ¥è€—æ—¶ä½œä¸ºå»¶è¿Ÿå‚è€ƒå€¼ï¼ˆå•ä½ï¼šæ¯«ç§’ï¼‰
    è¿”å› (proxy, latency_ms)ï¼Œè¶…æ—¶/å¤±è´¥è¿”å› (proxy, None)ã€‚
    """
    server = proxy.get("server", "")
    port = proxy.get("port", 0)
    name = proxy.get("name", "unknown")

    if not server or not port:
        return proxy, None

    try:
        port = int(port)
    except (ValueError, TypeError):
        return proxy, None

    # TCP è¿é€šæ€§æµ‹è¯•ï¼šè¿æ¥ä»£ç†æœåŠ¡å™¨
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(LATENCY_TIMEOUT)
        start = time.monotonic()
        sock.connect((server, port))
        latency_ms = (time.monotonic() - start) * 1000  # è½¬ä¸ºæ¯«ç§’
        sock.close()
        return proxy, round(latency_ms, 1)
    except (socket.timeout, socket.error, OSError):
        return proxy, None


def test_proxies_latency(proxies: list[dict]) -> list[dict]:
    """
    å¹¶å‘æµ‹è¯•æ‰€æœ‰ä»£ç†èŠ‚ç‚¹çš„å»¶è¿Ÿã€‚
    åˆ é™¤è¶…æ—¶ï¼ˆä¸å¯è¾¾ï¼‰çš„èŠ‚ç‚¹ï¼Œä¿ç•™çš„èŠ‚ç‚¹æŒ‰å»¶è¿Ÿä»ä½åˆ°é«˜æ’åºã€‚
    åœ¨èŠ‚ç‚¹åç§°å‰æ·»åŠ å»¶è¿Ÿæ ‡è®°ï¼Œä¾‹å¦‚ "[123ms] èŠ‚ç‚¹å"ã€‚
    """
    total = len(proxies)
    log.info(f"å¼€å§‹å»¶è¿Ÿæµ‹è¯•ï¼ˆå…± {total} ä¸ªèŠ‚ç‚¹ï¼Œè¶…æ—¶ {LATENCY_TIMEOUT}sï¼Œå¹¶å‘ {LATENCY_MAX_WORKERS} çº¿ç¨‹ï¼‰...")
    log.info(f"æµ‹è¯•ç›®æ ‡: TCP è¿æ¥åˆ°å„èŠ‚ç‚¹çš„ server:port")

    alive_proxies: list[tuple[dict, float]] = []  # (proxy, latency_ms)
    timeout_count = 0
    tested = 0

    with ThreadPoolExecutor(max_workers=LATENCY_MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
        future_to_proxy = {
            executor.submit(test_single_proxy, p): p for p in proxies
        }

        for future in as_completed(future_to_proxy):
            tested += 1
            proxy, latency = future.result()
            name = proxy.get("name", "unknown")

            if latency is not None:
                alive_proxies.append((proxy, latency))
                if tested % 20 == 0 or tested == total:
                    log.info(f"  æµ‹è¯•è¿›åº¦: {tested}/{total}  "
                             f"å­˜æ´»: {len(alive_proxies)}  è¶…æ—¶: {timeout_count}")
            else:
                timeout_count += 1
                if tested % 20 == 0 or tested == total:
                    log.info(f"  æµ‹è¯•è¿›åº¦: {tested}/{total}  "
                             f"å­˜æ´»: {len(alive_proxies)}  è¶…æ—¶: {timeout_count}")

    # æŒ‰å»¶è¿Ÿä»ä½åˆ°é«˜æ’åº
    alive_proxies.sort(key=lambda x: x[1])

    # åœ¨èŠ‚ç‚¹åç§°å‰æ·»åŠ å»¶è¿Ÿæ ‡è®°
    result: list[dict] = []
    for proxy, latency in alive_proxies:
        proxy["name"] = f"[{int(latency)}ms] {proxy['name']}"
        result.append(proxy)

    log.info(f"å»¶è¿Ÿæµ‹è¯•å®Œæˆ: å­˜æ´» {len(result)}/{total}ï¼Œ"
             f"è¶…æ—¶æ·˜æ±° {timeout_count} ä¸ªèŠ‚ç‚¹")

    if result:
        best = alive_proxies[0]
        worst = alive_proxies[-1]
        avg = sum(lat for _, lat in alive_proxies) / len(alive_proxies)
        log.info(f"  æœ€ä½å»¶è¿Ÿ: {int(best[1])}ms  "
                 f"æœ€é«˜å»¶è¿Ÿ: {int(worst[1])}ms  "
                 f"å¹³å‡å»¶è¿Ÿ: {int(avg)}ms")

    return result


def classify_by_region(proxies: list[dict]) -> dict[str, list[str]]:
    """
    æ ¹æ®èŠ‚ç‚¹åç§°ä¸­çš„å…³é”®è¯ï¼Œå°†èŠ‚ç‚¹åˆ†ç±»åˆ°å¯¹åº”çš„åœ°åŒºç»„ã€‚
    è¿”å› {åœ°åŒºç»„å: [èŠ‚ç‚¹åç§°åˆ—è¡¨]}ã€‚
    """
    region_groups: dict[str, list[str]] = OrderedDict()

    for proxy in proxies:
        name = proxy["name"]
        matched = False
        for pattern, region_name in REGION_MAP:
            if pattern.search(name):
                region_groups.setdefault(region_name, []).append(name)
                matched = True
                break  # ä¸€ä¸ªèŠ‚ç‚¹åªå½’å…¥ç¬¬ä¸€ä¸ªåŒ¹é…çš„åœ°åŒº
        # æœªåŒ¹é…ä»»ä½•åœ°åŒºçš„èŠ‚ç‚¹ä¸å½’å…¥åœ°åŒºç»„ï¼Œä½†ä»åœ¨"å…¨éƒ¨èŠ‚ç‚¹"ç»„ä¸­

    return region_groups


def build_config(proxies: list[dict]) -> dict:
    """
    åŸºäºæå–åˆ°çš„ä»£ç†èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ„å»ºå®Œæ•´çš„ Clash Meta é…ç½®ã€‚
    ç»“æ„å‚ç…§ list.meta.yml æ ·æœ¬ã€‚
    """
    # æ‰€æœ‰èŠ‚ç‚¹åç§°åˆ—è¡¨
    all_names = [p["name"] for p in proxies]

    # æŒ‰åœ°åŒºåˆ†ç±»
    region_groups = classify_by_region(proxies)

    # ---------- æ„å»º proxy-groups ----------
    proxy_groups: list[dict] = []

    # 1. ğŸš€ é€‰æ‹©ä»£ç† (æ€»å…¥å£)
    proxy_groups.append({
        "name": "ğŸš€ é€‰æ‹©ä»£ç†",
        "type": "select",
        "proxies": ["â™» è‡ªåŠ¨é€‰æ‹©", "ğŸ”° å»¶è¿Ÿæœ€ä½", "âœ… æ‰‹åŠ¨é€‰æ‹©", "ğŸ—ºï¸ é€‰æ‹©åœ°åŒº"],
    })

    # 2. â™» è‡ªåŠ¨é€‰æ‹© (fallback)
    proxy_groups.append({
        "name": "â™» è‡ªåŠ¨é€‰æ‹©",
        "type": "fallback",
        "url": "https://www.google.com/",
        "interval": 300,
        "proxies": list(all_names),  # å…¨éƒ¨èŠ‚ç‚¹
    })

    # 3. ğŸ”° å»¶è¿Ÿæœ€ä½ (url-test)
    proxy_groups.append({
        "name": "ğŸ”° å»¶è¿Ÿæœ€ä½",
        "type": "url-test",
        "url": "https://www.google.com/",
        "interval": 300,
        "tolerance": 20,
        "proxies": list(all_names),
    })

    # 4. âœ… æ‰‹åŠ¨é€‰æ‹©
    proxy_groups.append({
        "name": "âœ… æ‰‹åŠ¨é€‰æ‹©",
        "type": "select",
        "proxies": list(all_names),
    })

    # 5. ğŸŒ çªç ´é”åŒº
    proxy_groups.append({
        "name": "ğŸŒ çªç ´é”åŒº",
        "type": "select",
        "proxies": ["DIRECT", "ğŸš€ é€‰æ‹©ä»£ç†"],
    })

    # 6. â“ ç–‘ä¼¼å›½å†…
    proxy_groups.append({
        "name": "â“ ç–‘ä¼¼å›½å†…",
        "type": "select",
        "proxies": ["DIRECT", "ğŸš€ é€‰æ‹©ä»£ç†", "REJECT"],
    })

    # 7. ğŸŸ æ¼ç½‘ä¹‹é±¼
    proxy_groups.append({
        "name": "ğŸŸ æ¼ç½‘ä¹‹é±¼",
        "type": "select",
        "proxies": ["DIRECT", "ğŸš€ é€‰æ‹©ä»£ç†"],
    })

    # 8. ğŸš¨ ç—…æ¯’ç½‘ç«™
    proxy_groups.append({
        "name": "ğŸš¨ ç—…æ¯’ç½‘ç«™",
        "type": "select",
        "proxies": ["REJECT", "DIRECT"],
    })

    # 9. â›” å¹¿å‘Šæ‹¦æˆª
    proxy_groups.append({
        "name": "â›” å¹¿å‘Šæ‹¦æˆª",
        "type": "select",
        "proxies": ["REJECT", "DIRECT", "ğŸš€ é€‰æ‹©ä»£ç†"],
    })

    # 10. ğŸ—ºï¸ é€‰æ‹©åœ°åŒº (æ±‡é›†æ‰€æœ‰åœ°åŒºå­ç»„)
    active_region_names = [r for r in ALL_REGION_NAMES if r in region_groups]
    proxy_groups.append({
        "name": "ğŸ—ºï¸ é€‰æ‹©åœ°åŒº",
        "type": "select",
        "proxies": active_region_names if active_region_names else ["REJECT"],
    })

    # 11. å„åœ°åŒºå­ç»„
    for region_name in ALL_REGION_NAMES:
        if region_name in region_groups:
            proxy_groups.append({
                "name": region_name,
                "type": "select",
                "proxies": region_groups[region_name],
            })
        else:
            # æ— è¯¥åœ°åŒºèŠ‚ç‚¹æ—¶ä¿ç•™ç»„ä½†æ”¾å…¥ REJECT
            proxy_groups.append({
                "name": region_name,
                "type": "select",
                "proxies": ["REJECT"],
            })

    # ---------- æ„å»º rules ----------
    rules = build_rules()

    # ---------- æ„å»ºå®Œæ•´é…ç½® ----------
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    config = OrderedDict()
    config["# Update"] = None  # å ä½ï¼Œåç»­åœ¨è¾“å‡ºæ—¶æ‰‹åŠ¨å¤„ç†

    config_data = {
        "allow-lan": False,
        "dns": {
            "enable": True,
            "enhanced-mode": "redir-host",
            "fallback": ["8.8.8.8", "1.1.1.1"],
            "ipv6": True,
            "listen": ":1053",
            "nameserver": ["223.5.5.5", "114.114.114.114"],
        },
        "external-controller": "0.0.0.0:9090",
        "global-client-fingerprint": "chrome",
        "ipv6": True,
        "log-level": "warning",
        "mixed-port": 7890,
        "mode": "rule",
        "proxies": proxies,
        "proxy-groups": proxy_groups,
        "rules": rules,
        "sniffer": {
            "enable": True,
            "skip-domain": ["Mijia Cloud", "dlg.io.mi.com", "+.apple.com"],
            "sniff": {
                "HTTP": {
                    "override-destination": True,
                    "ports": [80, "8080-8880"],
                },
                "TLS": {
                    "ports": [443, 8443],
                },
            },
        },
        "tcp-concurrent": True,
        "unified-delay": True,
    }

    return config_data, now


def build_rules() -> list[str]:
    """
    æ„å»ºå¸¸ç”¨çš„åˆ†æµè§„åˆ™åˆ—è¡¨ã€‚
    åŒ…å«å¹¿å‘Šæ‹¦æˆªã€å›½å†…å¤–å¸¸è§åŸŸååˆ†æµã€GeoIP å…œåº•ç­‰ã€‚
    """
    rules = [
        # ---- å¹¿å‘Šæ‹¦æˆª ----
        "DOMAIN-SUFFIX,ads.google.com,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,adservice.google.com,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,googleadservices.com,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,doubleclick.net,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,ad.com,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,adnxs.com,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,adsrvr.org,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-SUFFIX,pgdt.ugdtimg.com,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-KEYWORD,adservice,â›” å¹¿å‘Šæ‹¦æˆª",
        "DOMAIN-KEYWORD,tracking,â›” å¹¿å‘Šæ‹¦æˆª",

        # ---- ç—…æ¯’/æ¶æ„ç½‘ç«™ ----
        "DOMAIN-SUFFIX,malware-site.example,ğŸš¨ ç—…æ¯’ç½‘ç«™",

        # ---- å›½å†…ç›´è¿ ----
        "DOMAIN-SUFFIX,cn,DIRECT",
        "DOMAIN-SUFFIX,baidu.com,DIRECT",
        "DOMAIN-SUFFIX,qq.com,DIRECT",
        "DOMAIN-SUFFIX,taobao.com,DIRECT",
        "DOMAIN-SUFFIX,tmall.com,DIRECT",
        "DOMAIN-SUFFIX,jd.com,DIRECT",
        "DOMAIN-SUFFIX,alipay.com,DIRECT",
        "DOMAIN-SUFFIX,163.com,DIRECT",
        "DOMAIN-SUFFIX,126.com,DIRECT",
        "DOMAIN-SUFFIX,weibo.com,DIRECT",
        "DOMAIN-SUFFIX,bilibili.com,DIRECT",
        "DOMAIN-SUFFIX,zhihu.com,DIRECT",
        "DOMAIN-SUFFIX,douyin.com,DIRECT",
        "DOMAIN-SUFFIX,toutiao.com,DIRECT",
        "DOMAIN-SUFFIX,csdn.net,DIRECT",
        "DOMAIN-SUFFIX,aliyun.com,DIRECT",
        "DOMAIN-SUFFIX,aliyuncs.com,DIRECT",
        "DOMAIN-SUFFIX,tencentcloud.com,DIRECT",
        "DOMAIN-SUFFIX,meituan.com,DIRECT",
        "DOMAIN-SUFFIX,dianping.com,DIRECT",
        "DOMAIN-SUFFIX,mi.com,DIRECT",
        "DOMAIN-SUFFIX,xiaomi.com,DIRECT",

        # ---- éœ€è¦ä»£ç†çš„æµ·å¤–æœåŠ¡ ----
        "DOMAIN-SUFFIX,google.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,google.co.jp,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,googleapis.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,gstatic.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,youtube.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,ytimg.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,googlevideo.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,gmail.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,github.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,githubusercontent.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,twitter.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,x.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,twimg.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,facebook.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,fbcdn.net,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,instagram.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,whatsapp.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,telegram.org,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,t.me,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,wikipedia.org,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,reddit.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,netflix.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,nflxvideo.net,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,spotify.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,discord.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,discordapp.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,openai.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,claude.ai,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,anthropic.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,chatgpt.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,amazonaws.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,cloudflare.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,microsoft.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,apple.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,icloud.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,amazon.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,twitch.tv,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,steam.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,steampowered.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,steamcommunity.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,pixiv.net,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,pximg.net,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,docker.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,docker.io,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,npmjs.org,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,pypi.org,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,huggingface.co,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,medium.com,ğŸš€ é€‰æ‹©ä»£ç†",
        "DOMAIN-SUFFIX,stackoverflow.com,ğŸš€ é€‰æ‹©ä»£ç†",

        # ---- æµåª’ä½“é”åŒº ----
        "DOMAIN-SUFFIX,hulu.com,ğŸŒ çªç ´é”åŒº",
        "DOMAIN-SUFFIX,hbo.com,ğŸŒ çªç ´é”åŒº",
        "DOMAIN-SUFFIX,hbomax.com,ğŸŒ çªç ´é”åŒº",
        "DOMAIN-SUFFIX,disneyplus.com,ğŸŒ çªç ´é”åŒº",
        "DOMAIN-SUFFIX,disney-plus.net,ğŸŒ çªç ´é”åŒº",
        "DOMAIN-SUFFIX,primevideo.com,ğŸŒ çªç ´é”åŒº",
        "DOMAIN-SUFFIX,dazn.com,ğŸŒ çªç ´é”åŒº",

        # ---- GeoIP å…œåº• ----
        "GEOIP,CN,â“ ç–‘ä¼¼å›½å†…",

        # ---- æœ€ç»ˆå…œåº• ----
        "MATCH,ğŸŸ æ¼ç½‘ä¹‹é±¼",
    ]
    return rules


def write_output(config_data: dict, update_time: str, filepath: Path) -> None:
    """
    å°†å®Œæ•´é…ç½®å†™å…¥ YAML æ–‡ä»¶ã€‚
    åœ¨æ–‡ä»¶å¤´éƒ¨æ·»åŠ æ›´æ–°æ—¶é—´æ³¨é‡Šã€‚
    """
    # è‡ªå®šä¹‰ YAML è¾“å‡ºæ ·å¼ï¼Œä½¿å­—ç¬¦ä¸²ä¸è¢«å¼ºåˆ¶åŠ å¼•å·
    class CleanDumper(yaml.SafeDumper):
        pass

    # å¤„ç† OrderedDict
    def _dict_representer(dumper, data):
        return dumper.represent_mapping("tag:yaml.org,2002:map", data.items())

    CleanDumper.add_representer(OrderedDict, _dict_representer)

    # ç¡®ä¿ä¸­æ–‡ç­‰ Unicode å­—ç¬¦æ­£å¸¸è¾“å‡º
    yaml_content = yaml.dump(
        config_data,
        Dumper=CleanDumper,
        default_flow_style=False,
        allow_unicode=True,
        sort_keys=False,
        width=1000,  # é¿å…é•¿è¡Œè¢«æŠ˜å 
    )

    # åœ¨é¡¶éƒ¨æ·»åŠ æ³¨é‡Š
    header = f"# Update: {update_time}\n"
    filepath.write_text(header + yaml_content, encoding="utf-8")
    log.info(f"é…ç½®æ–‡ä»¶å·²å†™å…¥: {filepath}")


# ======================== ä¸»æµç¨‹ ========================

def main():
    """è„šæœ¬ä¸»å…¥å£ï¼šè¯»å– â†’ æŠ“å– â†’ è§£æ â†’ å»é‡ â†’ å»¶è¿Ÿæµ‹è¯• â†’ åˆ†ç±» â†’ æ„å»º â†’ è¾“å‡º"""
    log.info("=" * 50)
    log.info("Clash Meta è®¢é˜…åˆå¹¶å·¥å…· å¯åŠ¨")
    log.info("=" * 50)

    # 1. è¯»å–è®¢é˜…é“¾æ¥
    urls = read_urls(ADDR_FILE)
    if not urls:
        log.error("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®¢é˜…é“¾æ¥ï¼Œé€€å‡º")
        sys.exit(1)

    # 2. é€ä¸ªæŠ“å–å¹¶æå–ä»£ç†èŠ‚ç‚¹
    all_proxies: list[dict] = []
    success_count = 0
    for i, url in enumerate(urls, 1):
        log.info(f"[{i}/{len(urls)}] å¤„ç†è®¢é˜…é“¾æ¥:")
        content = fetch_content(url)
        if content is None:
            log.warning(f"  è·³è¿‡è¯¥é“¾æ¥ï¼ˆè·å–å¤±è´¥ï¼‰")
            continue

        proxies = extract_proxies(content)
        if proxies:
            log.info(f"  æå–åˆ° {len(proxies)} ä¸ªèŠ‚ç‚¹")
            all_proxies.extend(proxies)
            success_count += 1
        else:
            log.warning(f"  æœªæå–åˆ°ä»»ä½•èŠ‚ç‚¹")

    log.info(f"å…±æˆåŠŸå¤„ç† {success_count}/{len(urls)} ä¸ªè®¢é˜…æº")
    log.info(f"å…±æå– {len(all_proxies)} ä¸ªåŸå§‹èŠ‚ç‚¹")

    if not all_proxies:
        log.error("æœªè·å–åˆ°ä»»ä½•ä»£ç†èŠ‚ç‚¹ï¼Œé€€å‡º")
        sys.exit(1)

    # 3. å»é‡
    proxies = deduplicate_proxies(all_proxies)
    log.info(f"å»é‡åå‰©ä½™ {len(proxies)} ä¸ªèŠ‚ç‚¹")

    # 4. è¿‡æ»¤ä¸­å›½å¤§é™†/é¦™æ¸¯/å°æ¹¾èŠ‚ç‚¹
    proxies = filter_blocked_regions(proxies)
    if not proxies:
        log.error("è¿‡æ»¤åæ— å‰©ä½™èŠ‚ç‚¹ï¼Œé€€å‡º")
        sys.exit(1)

    # 5. å»¶è¿Ÿæµ‹è¯•ï¼Œæ·˜æ±°è¶…æ—¶ä¸å¯è¾¾èŠ‚ç‚¹
    proxies = test_proxies_latency(proxies)
    if not proxies:
        log.error("æ‰€æœ‰èŠ‚ç‚¹å‡è¶…æ—¶ä¸å¯è¾¾ï¼Œé€€å‡º")
        sys.exit(1)

    # 6. æ„å»ºå®Œæ•´é…ç½®
    config_data, update_time = build_config(proxies)

    # 7. å†™å…¥æ–‡ä»¶
    write_output(config_data, update_time, OUTPUT_FILE)

    # 8. ç»Ÿè®¡åœ°åŒºåˆ†å¸ƒ
    region_groups = classify_by_region(proxies)
    log.info("--- åœ°åŒºåˆ†å¸ƒç»Ÿè®¡ ---")
    for region_name, names in region_groups.items():
        log.info(f"  {region_name}: {len(names)} ä¸ªèŠ‚ç‚¹")
    unclassified = len(proxies) - sum(len(v) for v in region_groups.values())
    if unclassified > 0:
        log.info(f"  â” æœªåˆ†ç±»: {unclassified} ä¸ªèŠ‚ç‚¹")

    log.info("=" * 50)
    log.info("å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: outcome.meta.yml")
    log.info("=" * 50)


if __name__ == "__main__":
    main()
